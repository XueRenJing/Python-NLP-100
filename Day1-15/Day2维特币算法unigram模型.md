
在Day1中提到最大匹配算法里面只利用了词典，并没有考虑到语义的情况，那么如果考虑语义的话，应该怎么做呢？

我们可以分为两步：

    第一步，产生所有分词的情况

    第二步，在所有分词的情况里面选最好的

在第二步中我们就需要考虑到语义，考虑到语义，就需要考虑语言模型。语言模型是什么呢？就是考虑这种分词的概率，例如把句子“我是好人”分词成（我，是，好人），像（我，是，好人）这其中一种分词发生的概率就可以用语言模型衡量。

为了简化问题，这里的语言模型考虑nigram模型，nigram模型指的是句子每个词发生的概率与周围词发生的概率无关，所以刚才的例子中的概率 P（我，是，好人）=P（我）* P（是）* P（好人）

因为刚才分成两步的话，效率较低，那么维特币这个人物就提出了维特币算法，维特币算法是语音识别、HMM的铺垫。

#### 给定一个例子，sentence为句子，wordlist为词典:
```python
'''
数据准备
例子：我想成为数据科学家
词典：['我','我想','想','成','成为','数','数据',‘科学’,'科学家','家']
概率x：[0.1,0.05,0.1,0.1,0.2,0.2,0.05,0.05,0.05,0.1]
'''
import numpy as np
from nltk.util import bigrams, trigrams
from nltk.text import Text
from nltk import FreqDist
from functools import reduce
import numpy as np

dictionary = ['我','我想','想','成','成为','数','数据','科学','科学家','家']
sentence = '我想成为数据科学家'
probability = [0.1,0.05,0.1,0.1,0.2,0.2,0.05,0.05,0.05,0.1]
```

#### 接下来我们为了实现维特币算法，需要做一个转换，概率先取对数，再对对数化之后的结果取负数，取负数的作用是能让我们求最小值，对数化的作用是把乘法的运算转换为加法运输。
#### 转化成加法运算之后，刚才提到的例子-log(P（我，是，好人))=(-log(P(我)))+(-log(P(是)))+(-log(P(好人))),通过加法的方式就能够实现维特币算法中的最短路径思想了:

```python
logx = -np.round(np.log(probability),2)
```
#### logx结果为：
```python
array([2.3 , 3.  , 2.3 , 2.3 , 1.61, 1.61, 3.  , 3.  , 3.  , 2.3 ])
```

#### 结合上面的分词方式以及每个词的概率进行-log(x)的转换后，可以通过下图的形式表达，例如路径上面词“我想”用点1到点3的路径表达，词“成为”用点3到点5的路径表达，词“我”用点1到点2的路径表达，如此类推。而上面的数字就是每个词概率的-log(x)转换
#### 一开始我们求各自分词的最大的概率，假设其中一种分词（‘我想’，‘成为’，‘数据’，‘科学’，‘家’）的
