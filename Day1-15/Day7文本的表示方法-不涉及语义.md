在机器学习任务中，我们需要吧数据转化为计算机能识别的数字，包括计算机视觉任务和自然语言处理任务。所以我们需要把文字表示成计算机能够进行识别运算的数字的形式，包括向量等等。下面对文本表示方法进行梳理，一般称为词嵌入（Word Embedding）方法。词嵌入这个说法其实很形象，我们的做法就是把文本中的词嵌入到文本空间中，用一个向量来表示它。通过我的理解，向量表示文本的方法包含涉及语义的和不涉及语义的。

## 不涉及语义的方法
### One-hot representation（表示方法）
One-hot比较好理解，首先给出语料库里面每个词一个编号进行索引，根据索引对词进行表示。那么就可以对词典里面的所有词进行表示，为了简单说明表示方法，我们举一个简单的词典例子：

> [我们，要注意，疫情，防控，做好，防护，措施]

首先对词信息编号

> {"我们": 1, "要注意": 2, "疫情": 3, "防控": 4, "做好": 5, "防护":6, "措施"}

在词向量中相应位置用1表示存在该词，其他位置表示不存在词，那么这些词就可以分别表示为：

>我们: [1,0,0,0,0,0,0]

>要注意: [0,1,0,0,0,0,0]

>疫情: [0,0,1,0,0,0,0]

>防控: [0,0,0,1,0,0,0]

>做好: [0,0,0,0,1,0,0]

>防护: [0,0,0,0,0,1,0]

>措施: [0,0,0,0,0,0,1]

#### python实现代码
```python
import pandas as pd
textlist = ['我们','要注意','疫情','防控','做好','防护','措施']
text1 = pd.get_dummies(textlist)

for i in textlist:
    print(i+':',text1[i].tolist())
```
能看出词的向量维度的个数等同于词典的长度，所以当词典很大时，每个词表示出来的词向量会比较稀疏。只有一个地方为1，其他地方都为0。容易造成维度灾难。

那么接下来句子的one-hot向量化应该怎么表示？
>疫情要注意卫生-> [疫情,要注意,卫生]-> [0,1,1,0,0,0,1]

>我们要注意防控措施-> [我们,要注意,防控,措施]-> [1,1,0,1,0,0,1]

以上的表达是怎么形成的？
“疫情要注意卫生”为例，词典中“我们”在句子中无法找到，用0表示，而词典中“要注意”在句子中能找到所以表示为1，接着，词典中“疫情”在句子中能够找到，表示为1。如此类推，把词典中的词进行遍历，就能够形成词向量。可以看出句子的one-hot向量化表示与词语的一致，都是从词典中找到相应的词在句子和词语中是否存在，存在则表示为1，不存在则表示为0。


### 词袋模型
除了用0和1是否存在的这种表达方式，还可以使用词典中的词在实际句子中的计数来表达句子的向量。所以这是一种把句子向量化的方法。

通过该方法，各种句子表示为：
>疫情要注意卫生-> [疫情,要注意,卫生]-> [0,1,1,0,0,0,1]
>我们要注意防控措施-> [我们,要注意,防控,措施]-> [1,1,0,1,0,0,1]
>我们要注意疫情防控，做好我们的防控措施-> [我们,要注意,疫情,防控,做好,我们,防控,措施]-> [2,1,1,1,2,0,1]

所以词频的方法就是通过计算词典中词在句子中的频次来表达某些词的重要性，词频更高的词相对重要性更高。
但是，词频越高代表的词不一定是越重要的词，以下面句子为例：
“我们是祖国未来的花朵，我们要好好学习报销祖国。”
在以上句子中“我们”这个词的词频比“学习”这个词的词频高，但实际上“学习”该词比“我们”的重要性更高，是这句话的核心含义。所以引入新的方法。

### TF-IDF算法
TF-IDF的分数代表了词语在当前文档和整个语料库中的相对重要性。是基于以上所说的词频的基础上增加计算了逆文档频率（Inverse Document Frequency），计算方式就是语料库中文档总数除以含有该词语的文档数量，然后再取对数。

为什么需要逆文档频率？是因为单纯地计算词频的话，会把一些不重要但是比较通用的词的重要性夸大。例如一些英文中的“a、the、at、in”和中文中的“吧，吗，因为”等词，这些词比较通用，但在我们的文档中实际上重要性可能没有那么高却词频很高。所以需要在词频的基础上增加逆文档频率来削弱它们的重要性。



#### python实现代码
```python
from sklearn.feature_extraction.text import TfidfVectorizer

cv=TfidfVectorizer(token_pattern=r"(?u)\b\w+\b")
vec=cv.fit_transform(['我 是 一条 天狗 呀','我 把 月 来 吞 了'])#传入句子组成的list
print(cv.vocabulary_)
print(vec)
```
#### 对应的优缺点：
>优点：
>相对简单、所以计算速度较快。
>缺点：
>通过词频的计算无法体现词的位置，位置不同的词计算出来的重要性一样，不够合理。换句话说，该方法没有理解词之间的关系。

介绍完上面文本向量化的方法后，再介绍文本向量化之后的在相似度方面的计算。文本相似度使用的领域较多，比较常用的为搜索引擎、问答系统等等。文本相似度的计算包括欧式距离、余弦相似度。

### 欧式距离

$$d=\sqrt(\sum_{i=1}^n (x_i-y_i)^2) $$


### 余弦相似度

$$\cos\theta=\frac{\sum_{i=1}^n x_i*y_i}{\sqrt{\sum_{i=1}^n x_i^2 \cdot \sum_{i=1}^n y_i^2}}$$

总的来说，以上的方法都有着各自的缺点，而共同的缺点都为无法理解文字的语义。所以后面诞生了兼顾理解文字语言的文本向量化的方法。

